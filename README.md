## SpOOx - Spatial Omics Oxford Analysis Pipeline
This pipeline has automated the analysis of mcd image files (generated by the [Fluidigm Hyperion Imaging System](https://www.fluidigm.com/products-services/instruments/hyperion)) using [imctools](https://github.com/BodenmillerGroup/imctools) and [DeepCell](https://simtk.org/projects/deepcell) for image processing and segmentation, and custom python and R scripts to extract image intensities for a variety of cell markers. The image intensities can be further analysed using [MLV - Multi-locus View](https://www.nature.com/articles/s42003-021-02097-y), and the associated images can be analysed using [Zegami](https://zegami.com/). The pipeline itself has been written using the python pipelining tool, [Ruffus](http://www.ruffus.org.uk).

### Set-up the pipeline ##################
The following notes assume that conda has been installed and in the system path - guidance on setting up conda in a Linux environment can be found [here](https://github.com/OBDS-Training/OBDS_Open_Workshop_Materials/blob/master/1_Conda/3_Conda_intro_CCB_Rcourse.md).

A zipped version of the SpOOx code can be [downloaded](https://github.com/Taylor-CCB-Group/SpOOx/archive/refs/heads/main.zip), or the repository can be cloned using:
```
git clone https://github.com/bioinfbloke/SpOOx.git
```
The conda environment is created using:
```
conda activate base
conda env create -n hyperion -f </path/to/>hyperion.yml
conda activate hyperion
```

### Run the pipeline ##################

Config:
```
cd <your_working_dir>

cp pipeline.yml .
# edit pipeline.yml params (eg, cluster queue name;  zegami options)
```

Input data - create a 'mcd' dir to contain all the .mcd data files to be analysed:
```
mkdir mcd 
```
Copy (or symlink) .mcd files to mcd/ dir. 

Each .mcd file should be in its own, named dir within the mcd dir. SpOOx and MDV relies on a specific naming scheme to process and parse files during the analysis. The file and dir names are constructed using three elements (\<condition\>, "sample", \<sampleId\>) separated by underscores. For example:
```
<your working dir>/mcd/COVID_sample_1/COVID_sample_1.mcd
<your working dir>/mcd/COVID_sample_2/COVID_sample_2.mcd
<your working dir>/mcd/INFLUENZA_sample_1/INFLUENZA_sample_1.mcd
<your working dir>/mcd/INFLUENZA_sample_2/INFLUENZA_sample_2.mcd
<your working dir>/mcd/INFLUENZA_sample_3/INFLUENZA_sample_3.mcd
<your working dir>/mcd/CONTROL_sample_1/CONTROL_sample_1.mcd
```
Subsequently, each region of interest (ROI) will have files named using this scheme:

\<condition\>, "sample", \<sampleId\>, "ROI", \<roi_id\>

For example:
```
COVID_sample_1_ROI_3.tiff
```
Similarly, in each row in the output data table will get a unique id:

\<condition\>, "sample", \<sampleId\>, "ROI", \<roi_id\>, "CELL", \<cell_id\>

For example:
```
COVID_sample_1_ROI_3_CELL_1.tiff
```

### Pipeline commands:
A basic Ruffus function 'show' can be used to describe the steps that make up the pipeline:
```
python hyperion_pipeline.py show
```
====================================

[imctools](https://github.com/BodenmillerGroup/imctools) works on folders not files, but folders are poor for tracking within a pipeline as their modicfication time is updated on access. To mitigate this we use a hidden dummy file (.ruffus) which is added to each directory of interest in the mcd dir for tracking purposes.

```
python hyperion_pipeline.py make mark_input_folders
```
====================================

The initial inputs to the pipeline are MCD image and metadata files from the hyperion and we want to convert these to OME_TIFF format files and extract each ROI for ease of processing. Each SAMPLE is put in a directory that contain the sub ROIs. There is other metadata extracted about running the machine but that is not used.
```
python hyperion_pipeline.py make mcd_to_tiff
```
====================================

The OME TIFF format files for each ROI are  processed to export a TIFF file based on each marker used as part of the panel. 
```
python hyperion_pipeline.py make tiff_to_histocat
```
====================================

Next we create a configuration file which is used to specify nuclear and cytoplasm channels which is used in segmentation next.
```
python hyperion_pipeline.py make make_config
```
Note - the config SHOULD be edited at this point before proceding to the next step

====================================

In the above configuration file you need to specify the markers by setting the respective column / row to 1 or 0. Nuclear and Cytoplasm single channel images are constructed and passed to Deep Cell to create a label matrix file for each ROI.
```
python hyperion_pipeline.py make deepcell
```
====================================

The following step uses the segmented cell masks generated by Deep Cell to extract average intensity information for each marker for each cell. This is then written to a tab separated file. Each cell has a row contains the cell id and the metadata associated with that cell including shape, marker name, etc.
```
python hyperion_pipeline.py make signal_extraction
```
====================================

Optional step for visualisation using Zegami (account required: https://zegami.com/)
```
python hyperion_pipeline.py make zegami_roi
```

====================================

### Final steps after pipeline:
Note: outside conda environment:

###  1. Clustering:
#### Step i)  Create a single file containing all data. 

Merge all of the cellData.tab files from each ROI under analysis using the script mergecelldata.py

This file can also filter the data prior to merging using the --minarea and --maxarea arguments
```
python mergecelldata.py
```
- `--indir` initial directory level (default=signalextraction)
  
- `--excludeList` list of dirs to exclude as a pattern if you have bad samples
   - (for example e.g. --excludeList AA_SAMPLE_11_ROI_1 BB_SAMPLE_4_ROI_12)
  
- `--infile` name of the files to be merged (default=cellData.tab)
  
- `--outfile` merged output file (default=mergecellData.tab)
  
- `--minarea` minimum value for area of each cell (default=50)
  
- `--maxarea` maximum value for area of each cell (default=300)
  
#### Step ii)  Create a metadata file.

(optional) For the clustering step (see below), a metadata file describing all the samples contained in the aboved merged data file can be created using the script, make_metadata.py

```
python make_metadata.py
```
- `--indir` the location of files used to generate this metadata (default=deepcell)
 
- `--outfile` output file (default=metadata.tsv)


#### Step iii)  Clustering.
```
Rscript Rphenoclustering.R \
--panel_file <markers.tsv> \
--metadata_file <metadata.tsv> \
--analysisName <analysis_name> \
--datatransf scaledtrim
--k 15 \
--q 0.001 \
--run_dimRed TRUE \
--out_dir <path_to_out_dir> \
--save_sceobj
```
====================================

### 2. Load data in Multi Spatial Viewier

See the instructions here
'''
https://github.com/Taylor-CCB-Group/SpOOx/tree/main/src/MLVUpload
'''
### 3. Spatial stats

\# [ not tested ] something like.....
```
python spatialstats.py -i StructuralIteration2.tab -o output -cl harmony_phenograph_exprs -c structural_iteration2.tab --roi HEALTHY_SAMPLE_1_ROI_1
```




